projects = [
    {
        'title_contents': [
            ['Generative AI, IntelliChat â€“ An Advanced LLM-Powered Conversational Legal Assistant', ['bold']]
        ],
        'subtitle_contents': None,
        'timeline': ['May 2024 - Present', []],
        'bullet_points': [
            [["Developed a modular conversational agent for a law firm to answer queries regarding legal documents and rules utilizing RAG techniques, integrating vector and graph databases for efficient document retrieval, and employing LangChain with Google Vertex AI.", []]],
            [["Built a React frontend for seamless user interactions, supporting text, voice, and image generation, while FastAPI backend ensures low-latency communication with various proprietary and open-source LLMs.", []]],
            [["Deployed the solution on Hugging Face Spaces with Docker, implementing robust RAG evaluation metrics (precision, recall, F1 score) to optimize performance, achieving significant improvements in retrieval accuracy and response quality.", []]]
        ]
    },
    {
        'title_contents': [
            ['Machine Learning, Harnessing Sequence Models for ASL Word Recognition', ['bold']]
        ],
        'subtitle_contents': None,
        'timeline': ['Mar 2023 - Apr 2023', []],
        'bullet_points': [
            [["Developed an advanced American Sign Language (ASL) word recognition system utilizing Transformers, LSTMs, and traditional ML models (SVM, KNN), achieving a 95% accuracy in classifying 250 sign classes from video inputs, leveraging comprehensive hyperparameter tuning with Optuna for optimal model performance.", []]],
            [["Streamlined data processing by reducing the dataset from 56GB to 12GB, enhancing model training efficiency and resource utilization, and conducted a robust comparative analysis to benchmark action recognition performance across various architectures, resulting in a 30% improvement in training speed.", []]]
        ]
    },
    {
        'title_contents': [
            ['Machine Learning, Customer Segmentation using Clustering Algorithms', ['bold']]
        ],
        'subtitle_contents': None,
        'timeline': ['May 2024 - Present', []],
        'bullet_points': [
            [["Leveraged various advanced clustering techniques such as hierarchical agglomerative clustering, Density Based Spatial Clustering, Gaussian Mixture Models, KD Trees, Latent Dirichlet Allocation, etc. to perform customer segmentation on real world retail chain data.", []]],
            [["Performed extensive data cleaning and preprocessing and dimensionality reduction techniques such as PCA and TSNE.", []]],
            [["Performed extensive data analysis and plotted various visualizations such as violin plots, kde plots, pair plots and used Inertia, Silhouette score and Davies-Bouldin Index as eval metrics.", []]],
            [["Developed marketing strategies targeting specific groups using insights from data.", []]],
            [["Implemented MLOPs pipeline to facilitate real data as it comes accounting for data drift and concept drift.", []]],
            [["Built the application using Streamlit and deployed it on HuggingFace spaces.", []]]
        ]
    },
    {
        'title_contents': [
            ['Multimodal Generative AI, Lightweight Visual Question Answering System', ['bold']]
        ],
        'subtitle_contents': None,
        'timeline': ['May 2024 - Present', []],
        'bullet_points': [
            [["Engineered a lightweight version of the LLaVA architecture by integrating the open-source Gemma model, achieving a 300% reduction in model size while maintaining comparable performance, and spearheading the development of a Lightweight Visual Question Answering (LVQA) system to enhance efficiency and scalability.", []]],
            [["Employed advanced NLP and image encoding techniques, integrating CLIP-ViT-L-336px with Gemma 2B Instruct, and pretrained a multimodal LLM on text-image pairs using Visual Instruction Tuning, resulting in benchmark evaluations that indicate the LVQA system's potential to exceed existing models in AI efficiency and accessibility by 25%.", []]]
        ]
    },
    {
        'title_contents': [
            ['AI for Human Computer Interaction, Alignment of LLMs to Human Preferences through RLHF', ['bold']]
        ],
        'subtitle_contents': None,
        'timeline': ['May 2024 - Present', []],
        'bullet_points': [
            [["Utilized the Reinforcement Learning through Human Feedback technique to align the LLaMa 7b model to produce outputs that align to human preferences by adjusting the weights through a feedback loop. This model was trained using the trl library from transformers.", []]],
            [["The outputs were evaluated on a range of metrics including toxicity, helpfulness and factual correctness. Parameter Efficient Finetuning methods such as quantization and LoRA were utilized.", []]]
        ]
    }
]

